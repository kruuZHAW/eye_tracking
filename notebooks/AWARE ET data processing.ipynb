{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_001 = 'C:/Users/halo/OneDrive - ZHAW/Python/AWARE mini-dataset/Project1 Data export_001.tsv'\n",
    "path_002 = 'C:/Users/halo/OneDrive - ZHAW/Python/AWARE mini-dataset/Project1 Data export_002.tsv'\n",
    "path_003 = 'C:/Users/halo/OneDrive - ZHAW/Python/AWARE mini-dataset/Project1 Data export_003.tsv'\n",
    "path_004 = 'C:/Users/halo/OneDrive - ZHAW/Python/AWARE mini-dataset/Project1 Data export_004.tsv'\n",
    "path_005 = 'C:/Users/halo/OneDrive - ZHAW/Python/AWARE mini-dataset/Project1 Data export_005.tsv'\n",
    "path_list = [path_001, path_002, path_003, path_004, path_005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(path):\n",
    "    file = pd.read_csv(path,sep='\\t')\n",
    "    file_date = pd.to_datetime(file['Recording timestamp'], unit='s').dt.strftime('%H:%M:%S.%f').str[:-3]\n",
    "    return file, file_date\n",
    "\n",
    "def task_range_finder(db, task_str):\n",
    "    all_task = db.loc[db['Event'].str.contains('Task', na=False)].index.to_list()\n",
    "    task_range_start = db.loc[db['Event'] == task_str].index.to_list()\n",
    "    task_range_end = [all_task[j+1] - 5*120\n",
    "        for j in range(len(all_task) - 1)  \n",
    "            if all_task[j] in task_range_start]\n",
    "    if len(task_range_start) != len(task_range_end):\n",
    "        task_range_end.append(all_task[-1] + 25*120)\n",
    "    return task_range_start, task_range_end\n",
    "\n",
    "def min_max_safe_transform(arr):\n",
    "    if arr.shape[0] == 0:\n",
    "        # Return empty 1D array if no samples\n",
    "        return np.array([])\n",
    "    # Fit a new scaler for each array\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(arr)  # shape => (rows, 1)\n",
    "    return scaled.ravel() \n",
    "def GP_array_list(db, task_start, task_end):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_mouse_x = []\n",
    "    all_mouse_y = []\n",
    "\n",
    "    for i in range(len(task_start)):\n",
    "        x = db['Gaze point X'].iloc[task_start[i]:task_end[i]].dropna().to_numpy().reshape(-1,1)\n",
    "        y = db['Gaze point Y'].iloc[task_start[i]:task_end[i]].dropna().to_numpy().reshape(-1,1)\n",
    "        mouse_x = db['Mouse position X'].iloc[task_start[i]:task_end[i]].dropna().to_numpy().reshape(-1,1)\n",
    "        mouse_y = db['Mouse position Y'].iloc[task_start[i]:task_end[i]].dropna().to_numpy().reshape(-1,1)\n",
    "        #print(x)\n",
    "        # Normalize each array (skip if empty)\n",
    "        x_scaled = min_max_safe_transform(x)\n",
    "        y_scaled = min_max_safe_transform(y)\n",
    "        mouse_x_scaled = min_max_safe_transform(mouse_x)\n",
    "        mouse_y_scaled = min_max_safe_transform(mouse_y)\n",
    "\n",
    "        # Append each result to the lists\n",
    "        all_x.append(x_scaled)\n",
    "        all_y.append(y_scaled)\n",
    "        all_mouse_x.append(mouse_x_scaled)\n",
    "        all_mouse_y.append(mouse_y_scaled)\n",
    "\n",
    "    # Return lists of arrays\n",
    "    return all_x, all_y, all_mouse_x, all_mouse_y\n",
    "def pad_or_truncate_nan(ts, target_length):\n",
    "    \"\"\"\n",
    "    Pads (or truncates) a 1D time-series array 'ts' with NaN \n",
    "    to make it exactly 'target_length' long.\n",
    "    \"\"\"\n",
    "    # Create an array of NaNs\n",
    "    padded = np.full(shape=(target_length,), fill_value=-1)\n",
    "    length = min(len(ts), target_length)\n",
    "    padded[:length] = ts[:length]  # copy up to target_length\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for i in range(len(path_list)):\n",
    "    file,_ = read_tsv(path_list[i])\n",
    "    all_files.append(file)\n",
    "\n",
    "# all task 1 in one list for training\n",
    "\n",
    "x_t1_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 1'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t1_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 1'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t1_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 1'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t1_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 1'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "\n",
    "#prepare for SVM\n",
    "\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 1) Find the longest time series\n",
    "    left_x_t1_max = max(len(ts) for ts in x_t1_list[i])\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt1_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t1_list[i]]\n",
    "    X_lyt1_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t1_list[i]]\n",
    "    X_mxt1_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t1_list[i]]  \n",
    "    X_myt1_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t1_list[i]]  \n",
    "    X_lt1 = np.vstack([X_lxt1_list, X_lyt1_list, X_mxt1_list, X_myt1_list])  # shape -> (n_series, max_len)\n",
    "y_lt1 = np.full(X_lt1.shape[0], 'Task1')\n",
    "\n",
    "# all task 2 in one list for training\n",
    "\n",
    "x_t2_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 2'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t2_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 2'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t2_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 2'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t2_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 2'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "#prepare for SVM\n",
    "\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt2_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t2_list[i]]\n",
    "    X_lyt2_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t2_list[i]]\n",
    "    X_mxt2_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t2_list[i]]  \n",
    "    X_myt2_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t2_list[i]]  \n",
    "    X_lt2 = np.vstack([X_lxt2_list, X_lyt2_list, X_mxt2_list, X_myt2_list])  # shape -> (n_series, max_len)\n",
    "y_t2 = np.full(X_lt2.shape[0], 'Task2')\n",
    "\n",
    "# all task 3 in one list for training\n",
    "\n",
    "x_t3_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 3'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t3_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 3'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t3_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 3'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t3_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 3'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt3_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t3_list[i]]\n",
    "    X_lyt3_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t3_list[i]]\n",
    "    X_mxt3_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t3_list[i]]  \n",
    "    X_myt3_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t3_list[i]]  \n",
    "    X_lt3 = np.vstack([X_lxt3_list, X_lyt3_list, X_mxt3_list, X_myt3_list])  # shape -> (n_series, max_len)\n",
    "y_t3 = np.full(X_lt3.shape[0], 'Task3')\n",
    "\n",
    "# all task 4 in one list for training\n",
    "\n",
    "x_t4_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 4'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t4_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 4'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t4_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 4'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t4_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 4'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt4_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t4_list[i]]\n",
    "    X_lyt4_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t4_list[i]]\n",
    "    X_mxt4_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t4_list[i]]  \n",
    "    X_myt4_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t4_list[i]]  \n",
    "    X_lt4 = np.vstack([X_lxt4_list, X_lyt4_list, X_mxt4_list, X_myt4_list])  # shape -> (n_series, max_len)\n",
    "y_t4 = np.full(X_lt4.shape[0], 'Task4')\n",
    "\n",
    "# all task 5 in one list for training\n",
    "\n",
    "x_t5_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 5'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t5_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 5'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t5_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 5'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t5_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 5'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt5_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t5_list[i]]\n",
    "    X_lyt5_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t5_list[i]]\n",
    "    X_mxt5_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t5_list[i]]  \n",
    "    X_myt5_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t5_list[i]]  \n",
    "    X_lt5 = np.vstack([X_lxt5_list, X_lyt5_list, X_mxt5_list, X_myt5_list])  # shape -> (n_series, max_len)\n",
    "y_t5 = np.full(X_lt5.shape[0], 'Task5')\n",
    "\n",
    "x_t6_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 6'))[0]\n",
    "    for file in all_files\n",
    "]\n",
    "y_t6_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 6'))[1]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_x_t6_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 6'))[2]\n",
    "    for file in all_files\n",
    "]\n",
    "mouse_y_t6_list = [\n",
    "    GP_array_list(file, *task_range_finder(file, 'Task 6'))[3]\n",
    "    for file in all_files\n",
    "]\n",
    "for i in range(len(x_t1_list)):\n",
    "    # 2) Pad all time-series to max_len with NaNs\n",
    "    X_lxt6_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in x_t6_list[i]]\n",
    "    X_lyt6_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in y_t6_list[i]]\n",
    "    X_mxt6_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_x_t6_list[i]]  \n",
    "    X_myt6_list = [pad_or_truncate_nan(ts, left_x_t1_max) for ts in mouse_y_t6_list[i]]  \n",
    "    X_lt6 = np.vstack([X_lxt6_list, X_lyt6_list, X_mxt6_list, X_myt6_list])  # shape -> (n_series, max_len)\n",
    "y_t6 = np.full(X_lt6.shape[0], 'Task6')\n",
    "\n",
    "# all tasks and labels in one array\n",
    "X_lt = np.vstack([X_lt1, X_lt2, X_lt3, X_lt4, X_lt5, X_lt6])\n",
    "y_t = np.hstack([y_lt1, y_t2, y_t3, y_t4, y_t5, y_t6])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
